{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nparslow/disfluency_gen/blob/develop/docs/tutorials/nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnxXKDjq3jEL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# refactored from:\n",
    "#https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "repoRoot = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(repoRoot, \"src\", \"disfluency_generator\"))\n",
    "\n",
    "import tensorflow as tf\n",
    "from machine_translator import load_data, create_dataset, print_examples, tf_lower_and_split_punct,\\\n",
    "    create_text_processor\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from trainTranslator import TrainTranslator, BatchLogs\n",
    "from maskedLoss import MaskedLoss\n",
    "\n",
    "from translator import Translator\n",
    "from trainTranslator import TrainTranslator\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from letsread_prepare_translations import LetsReadDataPrep\n",
    "from portuguese_phoneme_to_grapheme import PhonemeToGrapheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRVATYOgJs1b",
    "outputId": "0d4f0d79-6557-4c52-cb30-19c63d031a35",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = pathlib.Path(repoRoot, \"data\")\n",
    "verbose = 1\n",
    "#------------------\n",
    "letsread_corpus_path = os.path.join(data_path, \"LetsReadDB\")\n",
    "\n",
    "p2g = PhonemeToGrapheme(os.path.join(repoRoot, \"resources\", \"sampa.tsv\"))\n",
    "data_prep = LetsReadDataPrep(letsread_corpus_path, p2g)\n",
    "inputs, targets = data_prep.prep_letsread()\n",
    "\n",
    "\n",
    "if verbose > 0:\n",
    "    print(f\"Last example of data:\\n{inputs[-1]}\\n{targets[-1]}\")\n",
    "\n",
    "# we'll leave off the first 20 as a test set (todo improve)\n",
    "dataset = create_dataset(inputs[20:], targets[20:], BATCH_SIZE=64//2)\n",
    "\n",
    "if verbose > 0:\n",
    "    print(\"Printing Examples (before normalisation):\")\n",
    "    print_examples(dataset, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo - check with corpus:\n",
    "max_vocab_size = 2917\n",
    "\n",
    "input_text_processor = create_text_processor(inputs, max_vocab_size)\n",
    "\n",
    "if verbose > 0:\n",
    "    # todo better checking:\n",
    "    print(\"First 10 words of input vocab:\")\n",
    "    print(input_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "# note - we don't have to have the same output vocab size:\n",
    "output_text_processor = create_text_processor(targets, max_vocab_size)\n",
    "\n",
    "if verbose > 0:\n",
    "    print(\"First 10 words of target vocab:\")\n",
    "    print(output_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "if verbose > 0:\n",
    "    for example_input_batch, example_target_batch in dataset.take(1):\n",
    "        print(\"Example input token sequences (indices):\")\n",
    "        example_tokens = input_text_processor(example_input_batch)\n",
    "        print(example_tokens[:3, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzQWx2saImMV"
   },
   "source": [
    "Before getting into it define a few constants for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_a9uNz3-IrF-"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256//4\n",
    "units = 1024//4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI02XFjoEt1k"
   },
   "source": [
    "Now that you're confident that the training step is working, build a fresh copy of the model to train from scratch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "While there's nothing wrong with writing your own custom training loop, implementing the `Model.train_step` method, as in the previous section, allows you to run `Model.fit` and avoid rewriting all that boiler-plate code. \n",
    "\n",
    "This tutorial only trains for a couple of epochs, so use a `callbacks.Callback` to collect the history of batch losses, for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7m4mtnj80sq"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.01),  # default learning_rate = 0.001\n",
    "    loss=MaskedLoss(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQd_esVVoSf3",
    "outputId": "2536bbf4-078b-4f12-b0ac-c3d59ef81dfc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_translator.fit(dataset, epochs=50,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "38rLdlmtQHCm",
    "outputId": "efbeff93-c194-42ce-e124-36a36f88c350"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBQzFZ9uWU79"
   },
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyvxT5V0_X5B",
    "outputId": "039a2eea-9600-4df8-f83f-f89cba5f2f3b"
   },
   "outputs": [],
   "source": [
    "model_name = 'portugues_trial_3'\n",
    "tf.saved_model.save(translator, model_name,\n",
    "                    signatures={'serving_default': translator.tf_translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I0j3i3ekOba"
   },
   "outputs": [],
   "source": [
    "model_name = 'portugues_trial_3'\n",
    "reloaded = tf.saved_model.load(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXZF__FZXJCm",
    "outputId": "380eaf26-17ba-4dcd-8261-ed6e5da6e3ff"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "#three_input_text = tf.constant([\n",
    "#    # This is my life.\n",
    "#    'Esta es mi vida.',\n",
    "#    # Are they still home?\n",
    "#    '¿Todavía están en casa?',\n",
    "#    # Try to find out.'\n",
    "#    'Tratar de descubrir.',\n",
    "#])\n",
    "test_sentences = tf.constant(inputs[10:20])  # not used in training\n",
    "#test_sentences = tf.constant(inputs[20:30])  #targets[20:] is training\n",
    "\n",
    "result = reloaded.tf_translate(test_sentences)\n",
    "\n",
    "for orig, tr in zip(test_sentences, result['text']):\n",
    "    print(orig.numpy().decode())\n",
    "    print(tr.numpy().decode())\n",
    "    print(\"------------\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nmt_with_attention.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
