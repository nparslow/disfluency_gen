{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nparslow/disfluency_gen/blob/develop/docs/tutorials/nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnxXKDjq3jEL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# refactored from:\n",
    "#https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "repoRoot = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(repoRoot, \"src\", \"disfluency_generator\"))\n",
    "\n",
    "import tensorflow as tf\n",
    "from machine_translator import load_data, create_dataset, print_examples, tf_lower_and_split_punct,\\\n",
    "    create_text_processor\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from trainTranslator import TrainTranslator, BatchLogs\n",
    "from maskedLoss import MaskedLoss\n",
    "\n",
    "from translator import Translator\n",
    "from trainTranslator import TrainTranslator\n",
    "\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRVATYOgJs1b",
    "outputId": "0d4f0d79-6557-4c52-cb30-19c63d031a35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = pathlib.Path(\"/home/nickp/Documents/tutorials/disfluency_generator/data\")\n",
    "verbose = 1\n",
    "#------------------\n",
    "data_filename = pathlib.Path(data_path, \"spa-eng\", \"spa.txt\")\n",
    "\n",
    "targets, inputs = load_data(data_filename)\n",
    "\n",
    "if verbose > 0:\n",
    "    print(f\"Last example of data:\\n{inputs[-1]}\\n{targets[-1]}\")\n",
    "\n",
    "dataset = create_dataset(inputs, targets, BATCH_SIZE=64)\n",
    "\n",
    "if verbose > 0:\n",
    "    print_examples(dataset)\n",
    "\n",
    "# todo - check with corpus:\n",
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = create_text_processor(inputs, max_vocab_size)\n",
    "\n",
    "if verbose > 0:\n",
    "    # todo better checking:\n",
    "    print(\"First 10 words of input vocab:\")\n",
    "    print(input_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "# note - we don't have to have the same output vocab size:\n",
    "output_text_processor = create_text_processor(targets, max_vocab_size)\n",
    "\n",
    "if verbose > 0:\n",
    "    print(\"First 10 words of target vocab:\")\n",
    "    print(output_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "if verbose > 0:\n",
    "    for example_input_batch, example_target_batch in dataset.take(1):\n",
    "        print(\"Example input token sequences (indices):\")\n",
    "        example_tokens = input_text_processor(example_input_batch)\n",
    "        print(example_tokens[:3, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzQWx2saImMV"
   },
   "source": [
    "Before getting into it define a few constants for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_a9uNz3-IrF-"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI02XFjoEt1k"
   },
   "source": [
    "Now that you're confident that the training step is working, build a fresh copy of the model to train from scratch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "While there's nothing wrong with writing your own custom training loop, implementing the `Model.train_step` method, as in the previous section, allows you to run `Model.fit` and avoid rewriting all that boiler-plate code. \n",
    "\n",
    "This tutorial only trains for a couple of epochs, so use a `callbacks.Callback` to collect the history of batch losses, for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7m4mtnj80sq"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQd_esVVoSf3",
    "outputId": "2536bbf4-078b-4f12-b0ac-c3d59ef81dfc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_translator.fit(dataset, epochs=1,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "38rLdlmtQHCm",
    "outputId": "efbeff93-c194-42ce-e124-36a36f88c350"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBQzFZ9uWU79"
   },
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyvxT5V0_X5B",
    "outputId": "039a2eea-9600-4df8-f83f-f89cba5f2f3b"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(translator, 'translator',\n",
    "                    signatures={'serving_default': translator.tf_translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I0j3i3ekOba"
   },
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('translator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXZF__FZXJCm",
    "outputId": "380eaf26-17ba-4dcd-8261-ed6e5da6e3ff"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "three_input_text = tf.constant([\n",
    "    # This is my life.\n",
    "    'Esta es mi vida.',\n",
    "    # Are they still home?\n",
    "    '¿Todavía están en casa?',\n",
    "    # Try to find out.'\n",
    "    'Tratar de descubrir.',\n",
    "])\n",
    "\n",
    "result = reloaded.tf_translate(three_input_text)\n",
    "\n",
    "#%%time\n",
    "result = reloaded.tf_translate(three_input_text)\n",
    "\n",
    "for tr in result['text']:\n",
    "  print(tr.numpy().decode())\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nmt_with_attention.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
